Determining Complexity.

1-Q. 
What's the Big-O of the following algorithm? Submit your work and reasoning with your solution.

goodbye_world.rb
def goodbye_world(n)
 puts "Goodbye World! #{n}"
end

1-A.
Big-O: O(1)

In a worst case scenario, of n = infinity (while ignoring the practical processing and capacity limitations that would impose on real-world computing systems), this single statement in the function will run once. Since no matter how large the collection grows, the algorithm is completed in a fixed number of operations, this algorithm is of constant time complexity.

¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯

2-Q.
What's the Big-O of the following algorithm? Submit your work and reasoning with your solution.

find_largest.rb
def find_largest(collection)
 largest = collection[0]
 collection.length.times do |i|
   if collection[i] >= largest
     largest = collection[i]
   end
 end
 largest
end

2-A.
Big-O: O(n)

A worst case scenario for this search algorithm is that the largest item is last in the collection (note that in a best case scenario, this algorithm would still check every item in the collection, or put another way, Big-Oh = Big-Sigma). In that case, this algorithm will require checking every single element in the collection and comparing that element to the target, before at the end returning the largest item held in variable 'largest'.  We can say that this algorithm is of linear time complexity as the number of operations increases 1:1 with an increase in the size of the collection being searched in a worst case scenario. Note that we discard the operations of setting and returning single variables once for purposes of determining complexity.

¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯

3-Q.
What's the Big-O of the following algorithm? Submit your work and reasoning with your solution.

find_largest_2D_array.rb
def find_largest(collection)
 largest = collection[0][0]
 collection.length.times do |i|
   subcollection = collection[i]
   subcollection.length.times do |i|
     if subcollection[i] >= largest
       largest = subcollection[i]
     end
   end
 end
 largest
end

3-A.
Big-O: O(n)

This algorithm is identical to the one presented in question 2, with the added wrinkle that the collection is stored as a 2D or nested array. The total collection size n is equal to rows x columns. The algorithm is to iterate over every element in a column once before incrementing the row by one and iterating through that column and so on. So, this is no more work than if this was a single 1D array of length (row x column), and so is a Big-O of (n). 

If n was 4 arranged 2x2, operations would be 4. col[0][0], col[0][1], col[1][0], col[1][1].
If n was 9, arranged 3x3, operations would be 9. col[0][0], [0][1], [0][2], [1][0]..., [2]

¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯

4-Q.
What's the Big-O of the following algorithm? Submit your work and reasoning with your solution.

numbers_recurive.rb
def numbers(n)
 if (n == 0)
   return 0
 elsif (n == 1)
   return 1
 else
   return numbers(n-1) + numbers(n-2)
 end
end

4-A. Big-O(2^n)

While the answer looks deceptively like n^2 to n=10, at larger numbers n, the number of operations increases vastly more rapidly than the square of n. I counted the number of operations, charted it, and saw that the exponential complexity seems to fit this algorithm best. At n=30, the exponential rate is 407 times bigger than the calculated operations but this is far closer of an answer than the next closest n^2 which was 3,000 times smaller.

How I solved:

First, I did a few by hand.

For n = 0, the program executes 1 time, returning 1.
n = 1, the program also executes 1 time, returning 1. 
n = 2, the program executes 3 times, first hitting the else statement, then executing once for n-1 and again for n-2, returning 1.
n = 3, the program hits the else first, then runs twice, once for n = 3-1 and once for n=3-2. Now 3-1 resolves to two, which means it hits the else statement and then runs twice more, for a total of 5 operations.
n = 4, the program fires once, hits the else, and causes two executions of n=4-1 and n=4-2. When n = 3, we have two more executions of n=2 and n=1, where that n=2 triggers two more executions of code where n=1 and n=0. n=4-2 resolves to n=2, which will cause two additional code executions. This is a total of 9 executions of code.
n = 5, we hit an else, We execute twice, on n=4 and 3. For the n=4 branch, we execute twice again for n=3 and n=2.  For n=3, we execute twice more for n=2 and n=1, and twice more again for n=2. Back to the original n=2, we execute twice there as well. For the original n=3, we execute twice to get n=2 and n=1, and twice more for n=2's results of n=1 and n=0. This is a total of 15 operations for n=5.

After n=5, the math I was doing by hand was getting out of hand (pause for laughter-gasm) so I rewrote the method (see: fibonacci-recursive.rb) to tally the operations. I went 3 for 3 on the ones I also calculated by hand so I am confident in my methods. And the output is:

5 operations for n=3
9 operations for n=4
15 operations for n=5
25 operations for n=6
41 operations for n=7
67 operations for n=8
109 operations for n=9
177 operations for n=10
21891 operations for n=20
2692537 operations for n=30

Made into a chart of sorts, I am running with 2^n (exponential) as the best descriptor of Big-Oh of this algorithm.

n 	op 			n^2			2^n   n!
0		1				1				2			1
1		1				1				2			1
2		3				4				4			2		
3		5				9				8			6
4		9				16			16		24
5		15			25			32		120
6		25			36			64		720
7		41			49			128		5040
8		67			64			256		40320
9		81			81			512		362880
10	177			100			1024	3628800
20	21891		400			1.05m 2.4x10^8
30  2.7m		900			1.1b	2.6x10^32

¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯

5-Q. 
What's the Big-O of the following algorithm? Submit your work and reasoning with your solution.

numbers_iterative.rb
def iterative(n)
 num1 = 0
 num2 = 1

 i = 0
 while i < n-1
   tmp = num1 + num2
   num1 = num2
   num2 = tmp
   i+=1
 end

 num2
end

5-A. O(n)

Well, well, well. My good friend Fibonacci is back. This time we are calculating Fibonacci iteratively instead of recursively. We know from the benchmarks a bit back that iterative was far faster than recursive.

Let's try and do this by hand first.

n 		ops
0			1
1			1
2			2
3			3
4			4
5			5

This is an algorithm of linear complexity, as this table shows that the number of operations increases at the same rate as size of the collection n.

¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯¯\_(ツ)_/¯

6-Q.
What's the Big-O of the following algorithm? Submit your work and reasoning with your solution.

sort.rb
def sort(collection, from=0, to=nil)
 if to == nil
   # Sort the whole collection, by default
   to = collection.count - 1
 end

 if from >= to
   # Done sorting
   return
 end

 # Take a pivot value, at the far left
 pivot = collection[from]

 # Min and Max pointers
 min = from
 max = to

 # Current free slot
 free = min

 while min < max
   if free == min # Evaluate collection[max]
     if collection[max] <= pivot # Smaller than pivot, must move
       collection[free] = collection[max]
       min += 1
       free = max
     else
       max -= 1
     end
   elsif free == max # Evaluate collection[min]
     if collection[min] >= pivot # Bigger than pivot, must move
       collection[free] = collection[min]
       max -= 1
       free = min
     else
       min += 1
     end
   else
     raise "Inconsistent state"
   end
 end

 collection[free] = pivot

 quick_sort collection, from, free - 1
 quick_sort collection, free + 1, to

 collection
end

6-A. O(n^2)

This sort is using the quicksort algorithm, which I read has a Big-Oh of n-squared. However, let us test this out by hand to make a table to see if we can tease out the algorithmic complexity.

For a collection of length 1, we have a single operation, as `from >= to' causes a return for all single element arrays.

For n=2, say an array=[2,1], the while loop runs once, swapping the min and max, and then the function is called recurvisely and exits as from == to. The function returns a sorted list in two operations. 

For n=3 of array [3,2,1], we first iterate twice over the array in the while loop. Upon exiting the while loop, the pivot assignment sorts the array. However, we have to now pass through each recursion. The first recursion triggers an iteration in the while loop, then another recursion, which returns. Then we pass to the second of the two recursions, which returns right away. So in total we have 6 operations/iterations.

For n=4, array=[4,3,2,1], the while loop runs and we get [1,3,2,1] on the first iteration, then iterates twice more. After exiting the while loop, the pivot gets added back in to give us [1,3,2,4]. Then we call quick_sort (typo: should be sort()?) recursively on a subarray one element smaller than the original collection, so: [1,3,2]. 

In this loop, the while loop executes twice and exits before another recursive call on [1,3]. The while loop is again called, and does not change anything, runs once and exits, and calls the function recursively again with [1] as the subarray. This causes a return and we exit.

Then there is another recursive call, now on [3,2,4]. The while loop runs, first changing nothing, but on the second pass gives us [2,2,4] before we exit the loop. The array is set to [2,3,4] and hits the first recursive call, passing in [1,2,3,4] from index 2 to index 2, which causes a return. Now we execute the second recursive call, passing in [1,2,3,4], from index 3 to index 3. This also generates a return, exiting the call. Finally, the now sorted array is returned [1,2,3,4].

Here is our table.

n 		ops
1			1
2			2
3			6
4			13

As n increases, we approach n^2 for our worst case scenario Big-Oh. This makes sense, as we know that we are going to use a while loop on all of the elements on the array save 1 (n-1 iterations), and then with the recursions, we are going to recurse n times combined (n-1+1) in our examples. That gives us (n-1)n or n^2-n. If we plug in our values of n for n=3 and 4, we get this within one using this formula. Hopefully that is not a coincidence. When writing up Big-Oh, we drop the n because as we approach infinity it becomes irrelevant, so O(n^2).